{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Arqtic Weather Pipeline — Mission Presentation\n",
        "\n",
        "**Challenge**: Build a data pipeline that pulls weather data, processes it, and creates an interactive dashboard. Deploy to a major cloud provider using Infrastructure as Code.\n",
        "\n",
        "**Author**: Andre  \n",
        "**Date**: February 2026  \n",
        "**Stack**: Python 3.14 · DuckDB · Streamlit · Plotly · Prophet · Terraform · GCP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Architecture Overview\n",
        "\n",
        "![Architecture](../architecture.png)\n",
        "\n",
        "The solution follows a **local-first development model** — everything works on a laptop, then deploys to GCP as the final step.\n",
        "\n",
        "| Component | Technology | Rationale |\n",
        "|-----------|------------|----------|\n",
        "| Weather Data | Open-Meteo Forecast + Archive API | Free, no API key, WMO-backed accuracy |\n",
        "| Air Quality | Open-Meteo Air Quality API | Same provider, free, US AQI + PM2.5 hourly |\n",
        "| Data Tool | DuckDB + Parquet | Embedded SQL engine, zero-config, transparent GCS reads via httpfs |\n",
        "| Quality Gate | Pandera (11 validated fields) | DataFrame schema validation, catches bad data before storage |\n",
        "| Dashboard | Streamlit + Plotly | Interactive, Pythonic, drill-down with range selectors |\n",
        "| Forecasting | Prophet | Robust time-series with yearly seasonality and uncertainty bands |\n",
        "| IaC | Terraform | GCS, Cloud Run, Scheduler, IAM — all version-controlled |\n",
        "| CI/CD | GitHub Actions | Lint + test on PR, build + deploy on merge |\n",
        "| Package Manager | uv | 10x faster than pip, deterministic lockfile |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Pipeline — Live Demo\n",
        "\n",
        "The pipeline runs in 4 stages: **Extract → Validate → Transform → Load**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, os.path.join(os.getcwd(), \"..\"))\n",
        "\n",
        "# Run the full pipeline\n",
        "from pipeline.run import main\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Quality Gate\n",
        "\n",
        "Every row passes through Pandera schema validation before it reaches storage. Invalid data (temperatures > 60°C, negative wind speeds, humidity > 100%) is **rejected immediately**.\n",
        "\n",
        "**Critical discovery**: On Python 3.14, `import pandera as pa` fails with a `KeyError`. The correct import is `import pandera.pandas as pa`. This is undocumented — found through systematic testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pipeline.quality import DailyWeatherSchema\n",
        "\n",
        "# Show the schema definition\n",
        "print(\"Daily Weather Schema:\")\n",
        "print(DailyWeatherSchema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Demonstrate the quality gate catching bad data\n",
        "bad_data = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"date\": pd.Timestamp(\"2025-06-15\"),\n",
        "            \"temperature_2m_max\": 999.0,  # Impossible!\n",
        "            \"temperature_2m_min\": 15.0,\n",
        "            \"apparent_temperature_max\": 27.0,\n",
        "            \"apparent_temperature_min\": 13.0,\n",
        "            \"precipitation_sum\": 2.5,\n",
        "            \"wind_speed_10m_max\": 15.0,\n",
        "            \"relative_humidity_2m_mean\": 65.0,\n",
        "            \"weather_code\": 3.0,\n",
        "            \"daylight_duration\": 52000.0,\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "try:\n",
        "    DailyWeatherSchema.validate(bad_data, lazy=True)\n",
        "except Exception as e:\n",
        "    print(f\"Quality gate rejected: {type(e).__name__}\")\n",
        "    print(\"Reason: temperature_2m_max=999.0 exceeds range [-60, 60]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformation Layer — Making Weather Data Useful\n",
        "\n",
        "Raw weather numbers are meaningless to most people. The transform stage adds:\n",
        "\n",
        "1. **Thermal comfort labels**: Maps `apparent_temperature` to UTCI stress categories, then translates to everyday language: *\"Cold — Wear layers and a warm jacket.\"*\n",
        "2. **WMO weather descriptions**: Code 73 → \"Moderate snow ❄️\"\n",
        "3. **Wind gust labels**: Speed + gust thresholds → \"Windy — secure loose items\" or `None` when calm (silence = safe)\n",
        "4. **Visibility labels**: Meters → \"Low visibility — fog\" or `None` when clear\n",
        "5. **Anomaly detection**: Z-score against 30-day rolling mean\n",
        "6. **Historical comparison**: Today's temp vs multi-year average for this calendar date\n",
        "7. **Daylight hours**: Duration converted for clean display\n",
        "\n",
        "### Why apparent_temperature instead of pythermalcomfort?\n",
        "\n",
        "We investigated `pythermalcomfort` for scientifically precise UTCI calculations. However:\n",
        "- Open-Meteo's `apparent_temperature` already factors in wind chill and humidity\n",
        "- Mapping it to UTCI stress thresholds produces equivalent comfort categories\n",
        "- This removes 4 heavy transitive dependencies (numba, llvmlite, scipy, pythermalcomfort)\n",
        "- Docker image shrinks significantly; no LLVM compilation during build\n",
        "\n",
        "**Result**: Same user-facing outcome, simpler stack, faster builds.\n",
        "\n",
        "### Supplementary data sources (best-effort pattern)\n",
        "\n",
        "The pipeline also fetches two supplementary datasets using direct HTTP calls (bypassing the SDK):\n",
        "- **Sunrise/sunset times**: The Open-Meteo SDK can't deserialize ISO8601 string fields. Direct JSON parsing works. Used for accurate sunset countdown in the dashboard.\n",
        "- **Air quality (US AQI, PM2.5)**: Same Open-Meteo provider, free, no API key. Enables health-relevant alerts in the dashboard.\n",
        "\n",
        "Both are **best-effort** — if either API call fails, the pipeline continues and the dashboard gracefully degrades (those alerts simply don't appear)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pipeline.transform import COMFORT_TRANSLATIONS, _get_stress_category\n",
        "\n",
        "# Show the thermal comfort mapping\n",
        "test_temps = [-45, -35, -20, -5, 5, 15, 28, 35, 42, 50]\n",
        "\n",
        "print(f\"{'Apparent °C':>12} | {'Stress Category':<28} | {'Everyday Label':<18} | Advice\")\n",
        "print(\"-\" * 100)\n",
        "for t in test_temps:\n",
        "    cat = _get_stress_category(t)\n",
        "    label, advice, color = COMFORT_TRANSLATIONS[cat]\n",
        "    print(f\"{t:>12.0f} | {cat:<28} | {color} {label:<15} | {advice}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. DuckDB — The Right-Sized Data Tool\n",
        "\n",
        "DuckDB was chosen over BigQuery, Snowflake, or PostgreSQL because:\n",
        "- **Embedded**: No server, no credentials, zero-config\n",
        "- **Parquet-native**: Reads Parquet files directly with columnar pushdown\n",
        "- **GCS-transparent**: `httpfs` extension reads `gs://` paths in production\n",
        "- **SQL power**: Aggregation, window functions, and joins without loading full DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import duckdb\n",
        "\n",
        "# Query weather data directly from Parquet with SQL\n",
        "result = duckdb.query(\"\"\"\n",
        "    SELECT \n",
        "        condition_text,\n",
        "        COUNT(*) as days,\n",
        "        ROUND(AVG(temperature_2m_max), 1) as avg_high,\n",
        "        ROUND(AVG(temperature_2m_min), 1) as avg_low\n",
        "    FROM read_parquet('../data/daily/weather.parquet')\n",
        "    GROUP BY condition_text\n",
        "    ORDER BY days DESC\n",
        "    LIMIT 10\n",
        "\"\"\").df()\n",
        "\n",
        "print(\"Weather condition distribution (3 years):\")\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dashboard Highlights\n",
        "\n",
        "The Streamlit dashboard has 4 tabs:\n",
        "\n",
        "| Tab | Purpose | Key Features |\n",
        "|-----|---------|-------------|\n",
        "| Right Now | 3-second glanceable view | Hero display, smart alerts, Morning/Afternoon/Evening breakdown, weekly outlook |\n",
        "| Trends | Interactive time-series | Plotly with range selectors, anomaly markers, weekly/monthly aggregation via DuckDB |\n",
        "| Forecast | 30-day Prophet prediction | Uncertainty bands, comfort labels for predicted temperatures |\n",
        "| Data Quality | Pipeline health | Freshness indicator, validation status, anomaly log, data statistics |\n",
        "\n",
        "### \"Right Now\" Tab — Decision-Oriented Design\n",
        "\n",
        "The primary tab is designed for someone opening the dashboard at 7am before leaving the house. Instead of showing raw numbers, it synthesizes data into **actionable decisions**:\n",
        "\n",
        "- **Hero section**: Current condition, feels-like temperature, comfort advice, historical comparison as a subtle subtitle\n",
        "- **Smart alerts** (only appear when relevant — silence means safe):\n",
        "  - Precipitation: *\"Bring layers — light snow likely this morning\"* (uses temperature to distinguish rain vs snow)\n",
        "  - Visibility: *\"Low visibility this morning — drive carefully\"*\n",
        "  - Air quality: *\"Air quality is moderate (AQI 75) — sensitive groups should limit outdoor activity\"*\n",
        "  - UV: *\"High UV (8) — wear sunscreen\"*\n",
        "  - Sunset: *\"Sunset at 5:40pm (in ~11h 37m)\"* (uses real sunrise/sunset data)\n",
        "- **Your Day**: Morning (6-12), Afternoon (12-5), Evening (5-10) — each shows average temp, feels-like, dominant condition icon, comfort advice, and precipitation note if > 20% probability\n",
        "- **This Week**: 7-day outlook with comfort labels instead of low temperatures: *\"Wed ❄️ -7° · Cold\"*\n",
        "\n",
        "### Design Decisions\n",
        "- **`st.fragment`** on Forecast tab: Prophet training (~3s) only reruns when that specific tab is active\n",
        "- **`st.cache_data(ttl=600)`**: Parquet reads cached for 10 minutes\n",
        "- **Silence = safe**: Alerts only appear when action is needed. No wind alert today? Wind is fine.\n",
        "- **Temperature-based precipitation type**: If average temp ≤ 2°C, say \"snow\" not \"rain\" — more accurate than weather code mode\n",
        "\n",
        "Run the dashboard: `make run-dashboard` → http://localhost:8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Prophet Forecasting — Live Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "from forecast.predict import make_forecast\n",
        "\n",
        "# Load daily data and run forecast\n",
        "daily = pd.read_parquet(\"../data/daily/weather.parquet\")\n",
        "forecast = make_forecast(daily, metric_col=\"temperature_2m_max\", periods=30)\n",
        "\n",
        "# Show the next 7 days\n",
        "future = forecast[forecast[\"ds\"] > daily[\"date\"].max()].head(7)\n",
        "print(\"Next 7 days forecast:\")\n",
        "for _, row in future.iterrows():\n",
        "    cat = _get_stress_category(row[\"yhat\"])\n",
        "    label = COMFORT_TRANSLATIONS[cat][0]\n",
        "    print(\n",
        "        f\"  {row['ds'].strftime('%a %b %d')}: {row['yhat']:.1f}°C ({label}) \"\n",
        "        f\"[{row['yhat_lower']:.1f} to {row['yhat_upper']:.1f}]\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Infrastructure as Code (Terraform)\n",
        "\n",
        "All GCP resources are managed by Terraform:\n",
        "\n",
        "```\n",
        "terraform/\n",
        "├── provider.tf           # GCP provider + GCS backend for remote state\n",
        "├── variables.tf          # Configurable inputs (project, region, location)\n",
        "├── storage.tf            # GCS bucket for weather data\n",
        "├── artifact_registry.tf  # Docker image registry\n",
        "├── pipeline.tf           # Cloud Run Job + Cloud Scheduler\n",
        "├── dashboard.tf          # Cloud Run Service (public) + startup probe\n",
        "├── iam.tf                # Least-privilege service account\n",
        "├── outputs.tf            # Dashboard URL, bucket name\n",
        "└── tests/\n",
        "    └── main.tftest.hcl   # Native Terraform tests (plan-mode validation)\n",
        "```\n",
        "\n",
        "### Key decisions:\n",
        "- **Cloud Run** over GKE: Right-sized for a single dashboard + periodic job\n",
        "- **Cloud Scheduler** over Cloud Composer: No need for a full Airflow instance for one cron job\n",
        "- **GCS** over BigQuery: Parquet files are the perfect abstraction for this data volume\n",
        "- **Remote state** in GCS: Team-safe Terraform state management\n",
        "- **Native `tftest.hcl`**: Tests Terraform logic without deploying resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. SDLC, DataOps & DataQuality Practices\n",
        "\n",
        "The assessment asks for **at least one** — we implemented all three:\n",
        "\n",
        "### SDLC\n",
        "- Git flow: `main` + feature branches\n",
        "- Pre-commit hooks: ruff lint/format + terraform fmt/validate\n",
        "- CI: Automated lint + test on every PR\n",
        "- CD: Build + deploy on merge to main\n",
        "\n",
        "### DataOps\n",
        "- Scheduled pipeline execution (Cloud Scheduler, every 6 hours)\n",
        "- Environment-driven config (`DATA_PATH` switches local ↔ cloud)\n",
        "- Deterministic builds (`uv.lock`)\n",
        "- Multi-stage Docker for minimal images\n",
        "\n",
        "### DataQuality\n",
        "- **Quality gate**: Pandera schema validation — 11 hourly fields and 10 daily fields checked before data reaches storage\n",
        "- **Anomaly detection**: Z-score against 30-day rolling mean, visible in dashboard\n",
        "- **Freshness monitoring**: File modification time tracked in Data Quality tab\n",
        "- **Null tracking**: Zero nulls confirmed in data statistics\n",
        "- **Graceful degradation**: Supplementary data (AQI, sun times) is best-effort — pipeline and dashboard continue if those sources are unavailable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n",
        "\n",
        "result = subprocess.run(\n",
        "    [\"python\", \"-m\", \"pytest\", \"../tests/\", \"-v\", \"--tb=short\"], capture_output=True, text=True, cwd=\"..\"\n",
        ")\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(result.stderr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Production Considerations\n",
        "\n",
        "| Area | Current | Production Enhancement |\n",
        "|------|---------|----------------------|\n",
        "| Auth | SA key in GitHub secret | Workload Identity Federation (OIDC) |\n",
        "| Monitoring | Data Quality tab | Cloud Monitoring alerts on pipeline failures |\n",
        "| Scaling | Single Cloud Run instance | Autoscaling with min instances for cold start |\n",
        "| Multi-location | Single city (Toronto) | Config-driven, one pipeline per city |\n",
        "| Data retention | 90-day GCS lifecycle | Archival to Coldline for historical analysis |\n",
        "| Secret management | Environment variables | Secret Manager integration |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Cost Considerations\n",
        "\n",
        "| Resource | Usage Pattern | Cost Expectation |\n",
        "|----------|--------------|-----------------|\n",
        "| Cloud Run Service | Low-traffic dashboard, scales to zero | Minimal (within free tier) |\n",
        "| Cloud Run Job | 4 short runs/day | Minimal (within free tier) |\n",
        "| GCS Storage | Small Parquet files | Minimal (within free tier) |\n",
        "| Artifact Registry | Single Docker image | Negligible |\n",
        "| Cloud Scheduler | 4 triggers/day | Free (3 free jobs included) |\n",
        "\n",
        "The solution was deliberately designed to fit within GCP's free tier — all components are right-sized to avoid unnecessary cost while maintaining full functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This solution demonstrates:\n",
        "- **End-to-end pipeline**: Extract → Validate → Transform → Load → Visualize (4 data sources, 11 validated fields)\n",
        "- **Practical engineering**: Right-sized tools (DuckDB, not BigQuery), no over-engineering\n",
        "- **Human-centered design**: Weather data translated to actionable decisions — not raw numbers but \"bring an umbrella\", \"wear layers\", \"air quality is moderate\"\n",
        "- **Complementary data sources**: Air quality, sunrise/sunset, wind gusts, visibility — all free, all from Open-Meteo, all adding genuine value\n",
        "- **Quality-first**: Pandera schema validation, z-score anomaly detection, freshness monitoring\n",
        "- **Production-ready IaC**: Terraform-managed GCP with CI/CD automation, native `.tftest.hcl` tests\n",
        "- **29 passing tests** (unit + integration E2E), zero lint errors, deterministic builds with `uv.lock`\n",
        "\n",
        "All code: [github.com/andreibuaka/arqtic](https://github.com/andreibuaka/arqtic)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}